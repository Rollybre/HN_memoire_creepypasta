iris
library(syuzhet)
# Fonction pour diviser un texte en parties
split_text_into_n_parts <- function(text, n) {
total_length <- nchar(text)
part_length <- total_length %/% n
extra_chars <- total_length %% n
parts <- vector("character", n)
start_index <- 1
for (i in 1:n) {
current_part_length <- part_length + ifelse(i <= extra_chars, 1, 0)
parts[i] <- substr(text, start_index, start_index + current_part_length - 1)
start_index <- start_index + current_part_length
}
return(parts)
}
# Fonction pour analyser le sentiment d'un texte
analyze_sentiment <- function(text) {
chunked <- split_text_into_n_parts(text, 100)
sentiments <- get_sentiment(chunked, method="bing")
sentiments_tr <-get_dct_transform(sentiments)
return(sentiments_tr)
}
# Chemin vers le dossier contenant les fichiers texte
dossier <- "../17.04_txt/reddit_no_sleep/"
# Liste des fichiers texte dans le dossier
fichiers <- list.files(dossier, pattern = "\\.txt$", full.names = TRUE)
setwd("~/Documents/10-19_Université_et_scolarité/17_Memoire/17.09_R")
# Initialisation d'une liste pour stocker les résultats de sentiment pour chaque fichier
resultats_sentiment <- list()
# Analyse du sentiment pour chaque fichier
library(progress)
pb <- progress_bar$new(format = "[:bar] :percent :eta", total = length(fichiers))
# Boucle sur chaque fichier
for (fichier in fichiers) {
texte <- readLines(fichier)
texte <- paste(texte, collapse=' ')
# Analyse du sentiment
resultats_sentiment[[fichier]] <- analyze_sentiment(texte)
# Mettre à jour la barre de progression
pb$tick()
}
chunked <- split_text_into_n_parts(text, 100)
# Fonction pour analyser le sentiment d'un texte
analyze_sentiment <- function(text) {
chunked <- split_text_into_n_parts(text, 100)
sentiments <- get_sentiment(chunked, method="bing")
sentiments_tr <-get_dct_transform(sentiments)
return(sentiments_tr)
}
# Chemin vers le dossier contenant les fichiers texte
dossier <- "../17.04_txt/reddit_no_sleep/"
# Liste des fichiers texte dans le dossier
fichiers <- list.files(dossier, pattern = "\\.txt$", full.names = TRUE)
# Initialisation d'une liste pour stocker les résultats de sentiment pour chaque fichier
resultats_sentiment <- list()
# Analyse du sentiment pour chaque fichier
library(progress)
pb <- progress_bar$new(format = "[:bar] :percent :eta", total = length(fichiers))
# Boucle sur chaque fichier
for (fichier in fichiers) {
texte <- readLines(fichier)
texte <- paste(texte, collapse=' ')
# Analyse du sentiment
resultats_sentiment[[fichier]] <- analyze_sentiment(texte)
# Mettre à jour la barre de progression
pb$tick()
}
# Finaliser la barre de progression
pb$close()
# Normalisation des valeurs de sentiment
max_sentiment <- max(unlist(lapply(resultats_sentiment, max)))
min_sentiment <- min(unlist(lapply(resultats_sentiment, min)))
# Tracé des graphiques
par(mfrow=c(1,1)) # Ajustement de la disposition des graphiques
colors <- rainbow(length(resultats_sentiment)) # Couleurs pour les différentes courbes
legend_names <- c() # Initialisation pour stocker les noms des fichiers pour la légende
plot(NULL, xlim = c(1, 100), ylim = c(0, 1),
type = "l", main = "Analyse de sentiment",
xlab = "Narrative Time", ylab = "Emotional Valence")
resultats_sentiment_tr<-list()
for (i in 1:length(resultats_sentiment)) {
tr_sentiment <- get_dct_transform(resultats_sentiment[[i]])
tr_sentiment <- scale(tr_sentiment, center = min_sentiment, scale = max_sentiment - min_sentiment)
resultats_sentiment_tr[[i]] <- tr_sentiment
x <- 1:length(tr_sentiment)
lines(x, tr_sentiment, col = colors[i])
legend_names <- c(legend_names, basename(names(resultats_sentiment)[i]))
}
# Calcul des distances euclidiennes entre chaque paire de vecteurs
distances <- dist(do.call(rbind, resultats_sentiment))
# Convertir les distances en une matrice carrée
dist_matrix <- as.matrix(distances)
# Tracez un histogramme des distances ou un graphique de densité
plot(density(distances))
View(dist_matrix)
library(syuzhet)
# Fonction pour diviser un texte en parties
split_text_into_n_parts <- function(text, n) {
total_length <- nchar(text)
part_length <- total_length %/% n
extra_chars <- total_length %% n
parts <- vector("character", n)
start_index <- 1
for (i in 1:n) {
current_part_length <- part_length + ifelse(i <= extra_chars, 1, 0)
parts[i] <- substr(text, start_index, start_index + current_part_length - 1)
start_index <- start_index + current_part_length
}
return(parts)
}
# Fonction pour analyser le sentiment d'un texte
analyze_sentiment <- function(text) {
chunked <- split_text_into_n_parts(text, 100)
sentiments <- get_sentiment(chunked, method="bing")
sentiments_tr <-get_dct_transform(sentiments)
return(sentiments_tr)
}
# Chemin vers le dossier contenant les fichiers texte
dossier <- "/Users/rolly/Documents/10-19_Université_et_scolarité/17_Memoire/17.04_txt/corpus"
# Liste des fichiers texte dans le dossier
fichiers <- list.files(dossier, pattern = "\\.txt$", full.names = TRUE)
library(syuzhet)
# Fonction pour diviser un texte en parties
split_text_into_n_parts <- function(text, n) {
total_length <- nchar(text)
part_length <- total_length %/% n
extra_chars <- total_length %% n
parts <- vector("character", n)
start_index <- 1
for (i in 1:n) {
current_part_length <- part_length + ifelse(i <= extra_chars, 1, 0)
parts[i] <- substr(text, start_index, start_index + current_part_length - 1)
start_index <- start_index + current_part_length
}
return(parts)
}
# Fonction pour analyser le sentiment d'un texte
analyze_sentiment <- function(text) {
chunked <- split_text_into_n_parts(text, 100)
sentiments <- get_sentiment(chunked, method="bing")
sentiments_tr <-get_dct_transform(sentiments)
return(sentiments_tr)
}
# Chemin vers le dossier contenant les fichiers texte
dossier <- "/Users/rolly/Documents/10-19_Université_et_scolarité/17_Memoire/17.04_txt/corpus"
# Liste des fichiers texte dans le dossier
fichiers <- list.files(dossier, pattern = "\\.txt$", full.names = TRUE)
# Initialisation d'une liste pour stocker les résultats de sentiment pour chaque fichier
resultats_sentiment <- list()
# Analyse du sentiment pour chaque fichier
library(progress)
pb <- progress_bar$new(format = "[:bar] :percent :eta", total = length(fichiers))
# Boucle sur chaque fichier
for (fichier in fichiers) {
texte <- readLines(fichier)
texte <- paste(texte, collapse=' ')
# Analyse du sentiment
resultats_sentiment[[fichier]] <- analyze_sentiment(texte)
# Mettre à jour la barre de progression
pb$tick()
}
# Normalisation des valeurs de sentiment
max_sentiment <- max(unlist(lapply(resultats_sentiment, max)))
min_sentiment <- min(unlist(lapply(resultats_sentiment, min)))
for (i in 1:length(resultats_sentiment)) {
tr_sentiment <- get_dct_transform(resultats_sentiment[[i]])
tr_sentiment <- scale(tr_sentiment, center = min_sentiment, scale = max_sentiment - min_sentiment)
resultats_sentiment_tr[[i]] <- tr_sentiment
x <- 1:length(tr_sentiment)
}
resultats_sentiment_tr<-list()
for (i in 1:length(resultats_sentiment)) {
tr_sentiment <- get_dct_transform(resultats_sentiment[[i]])
tr_sentiment <- scale(tr_sentiment, center = min_sentiment, scale = max_sentiment - min_sentiment)
resultats_sentiment_tr[[i]] <- tr_sentiment
x <- 1:length(tr_sentiment)
}
vecteur_moyen <- numeric(length(liste_vecteurs[[1]]))
vecteur_moyen <- numeric(length(resultats_sentiment_tr[[1]]))
# Calculer la somme des vecteurs
for (v in liste_vecteurs) {
vecteur_moyen <- vecteur_moyen + v
}
# Calculer la somme des vecteurs
for (v in resultats_sentiment_tr) {
vecteur_moyen <- vecteur_moyen + v
}
# Diviser par le nombre total de vecteurs
vecteur_moyen <- vecteur_moyen / length(liste_vecteurs)
# Diviser par le nombre total de vecteurs
vecteur_moyen <- vecteur_moyen / length(resultats_sentiment_tr)
View(vecteur_moyen)
# Installer les packages si vous ne les avez pas déjà installés
install.packages("proxy")
install.packages("textreuse")
# Charger les bibliothèques nécessaires
library(dplyr)
# Lire le fichier CSV
data <- read.csv("/Users/rolly/Documents/10-19_Université_et_scolarité/17_Memoire/17.02_CSV.CSV/resultats/data_complet_final.csv")
# Extraire la colonne contenant les textes
texte <- data$Texte
# Définir la taille de l'échantillon
sample_size <- 10 # par exemple, changer selon vos besoins
# Faire un échantillonnage aléatoire des textes
set.seed(123) # pour rendre l'échantillonnage reproductible
sampled_texts <- sample(texte, sample_size)
# Créer un sous-dossier pour les fichiers texte échantillonnés
dir.create("sampled_texts")
# Exporter chaque texte échantillonné dans un fichier texte individuel
for (i in 1:length(sampled_texts)) {
writeLines(sampled_texts[i], paste0("sampled_texts/text_", i, ".txt"))
}
print("Échantillonnage et exportation terminés.")
source("~/Documents/10-19_Université_et_scolarité/17_Memoire/17.09_R/text_reuse.R")
# Charger les bibliothèques nécessaires
library(dplyr)
# Lire le fichier CSV
data <- read.csv("/Users/rolly/Documents/10-19_Université_et_scolarité/17_Memoire/17.02_CSV.CSV/resultats/data_complet_final.csv")
# Extraire la colonne contenant les textes
texte <- data$Texte
# Définir la taille de l'échantillon
sample_size <- 10 # par exemple, changer selon vos besoins
# Faire un échantillonnage aléatoire des textes
set.seed(123) # pour rendre l'échantillonnage reproductible
sampled_texts <- sample(texte, sample_size)
# Créer un sous-dossier pour les fichiers texte échantillonnés
dir.create("sampled_texts")
# Exporter chaque texte échantillonné dans un fichier texte individuel
for (i in 1:length(sampled_texts)) {
writeLines(sampled_texts[i], paste0("sampled_texts/text_", i, ".txt"))
}
print("Échantillonnage et exportation terminés.")
setwd("~/Documents/10-19_Université_et_scolarité/17_Memoire/17.09_R")
# Charger les bibliothèques nécessaires
library(dplyr)
# Lire le fichier CSV
data <- read.csv("/Users/rolly/Documents/10-19_Université_et_scolarité/17_Memoire/17.02_CSV.CSV/resultats/data_complet_final.csv")
# Extraire la colonne contenant les textes
texte <- data$Texte
# Définir la taille de l'échantillon
sample_size <- 10 # par exemple, changer selon vos besoins
# Faire un échantillonnage aléatoire des textes
set.seed(123) # pour rendre l'échantillonnage reproductible
sampled_texts <- sample(texte, sample_size)
# Créer un sous-dossier pour les fichiers texte échantillonnés
dir.create("sampled_texts")
# Exporter chaque texte échantillonné dans un fichier texte individuel
for (i in 1:length(sampled_texts)) {
writeLines(sampled_texts[i], paste0("sampled_texts/text_", i, ".txt"))
}
print("Échantillonnage et exportation terminés.")
## text reuse
dir <- system.file("sampled_texts", package = "textreuse")
minhash <- minhash_generator(200, seed = 235)
library(textreuse)
## text reuse
dir <- system.file("sampled_texts", package = "textreuse")
minhash <- minhash_generator(200, seed = 235)
ats <- TextReuseCorpus(dir = dir,
tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash)
?TextReuseCorpus
?system.file
## text reuse
dir <- "sampled_texts"
minhash <- minhash_generator(200, seed = 235)
ats <- TextReuseCorpus(dir = dir,
tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash)
View(ats)
buckets <- lsh(ats, bands = 50, progress = FALSE)
candidates <- lsh_candidates(buckets)
scores <- lsh_compare(candidates, ats, jaccard_similarity, progress = FALSE)
scores
View(scores)
?lsh_candidate
?lsh_candidates
?lsh
## text reuse
dir <- "/Users/rolly/Documents/10-19_Université_et_scolarité/17_Memoire/17.04_txt/corpus"
minhash <- minhash_generator(200, seed = 235)
ats <- TextReuseCorpus(dir = dir,
tokenizer = tokenize_ngrams, n = 5,
minhash_func = minhash)
warnings()
buckets <- lsh(ats, bands = 50, progress = FALSE)
buckets <- lsh(ats, bands = 50, progress = TRUE)
candidates <- lsh_candidates(buckets)
scores <- lsh_compare(candidates, ats, jaccard_similarity, progress = FALSE)
scores
?scores
?lsh_compare
a <- "'How do I know', she asked, 'if this is a good match?'"
b <- "'This is a match', he replied."
align_local(a, b)
scores[1]
scores[1,]
## On parcours le df
chemin_texte_1<- paste0("sampled_texts/",score[1,1],'txt')
## On parcours le df
chemin_texte_1<- paste0("sampled_texts/",scores[1,1],'txt')
chemin_texte_2<- paste0("sampled_texts/",scores[1,2],'txt')
## On parcours le df
chemin_texte_1<- paste0("sampled_texts/",scores[1,1],'.txt')
chemin_texte_2<- paste0("sampled_texts/",scores[1,2],'.txt')
## On y accèse et on stock dans une variable
texte_1<-readLines(chemin_texte_1)
## On parcours le df et on isole les chemins
chemin_texte_1<- paste0("dir","/",scores[1,1],'.txt')
## On parcours le df et on isole les chemins
chemin_texte_1<- paste0(dir,"/",scores[1,1],'.txt')
chemin_texte_1<- paste0(dir,"/",scores[1,2],'.txt')
chemin_texte_2<- paste0(dir,"/",scores[1,2],'.txt')
## On parcours le df et on isole les chemins
chemin_texte_1<- paste0(dir,"/",scores[1,1],'.txt')
chemin_texte_2<- paste0(dir,"/",scores[1,2],'.txt')
## On y accèse et on stock dans une variable
texte_1<-readLines(chemin_texte_1)
texte_2<-readLines(chemin_texte_2)
## On compare !
align_local(texte_1,texte_2)
texte_1
## On y accèse et on stock dans une variable
texte_1<-paste(readLines(chemin_texte_1), collapse=" ")
texte_2<-paste(readLines(chemin_texte_2), collapse=" ")
## On parcours le df et on isole les chemins
chemin_texte_1<- paste0(dir,"/",scores[1,1],'.txt')
chemin_texte_2<- paste0(dir,"/",scores[1,2],'.txt')
chemin_texte_1
chemin_texte_2
## On y accèse et on stock dans une variable
texte_1<-paste(readLines(chemin_texte_1), collapse=" ")
texte_2<-paste(readLines(chemin_texte_2), collapse=" ")
texte_1
## On compare !
align_local(texte_1,texte_2)
## On parcours le df et on isole les chemins
chemin_texte_1<- paste0(dir,"/",scores[6,1],'.txt')
chemin_texte_2<- paste0(dir,"/",scores[6,2],'.txt')
## On y accèse et on stock dans une variable
texte_1<-paste(readLines(chemin_texte_1), collapse=" ")
texte_2<-paste(readLines(chemin_texte_2), collapse=" ")
scores
## On parcours le df et on isole les chemins
chemin_texte_1<- paste0(dir,"/",scores[7,1],'.txt')
chemin_texte_2<- paste0(dir,"/",scores[7,2],'.txt')
## On y accèse et on stock dans une variable
texte_1<-paste(readLines(chemin_texte_1), collapse=" ")
texte_2<-paste(readLines(chemin_texte_2), collapse=" ")
## On compare !
align_local(texte_1,texte_2)
source("~/Documents/10-19_Université_et_scolarité/17_Memoire/17.09_R/text_reuse.R")
